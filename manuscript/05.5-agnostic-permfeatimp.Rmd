```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```


## Feature Importance {#feature-importance}
A feature's importance is the increase in the model's prediction error after we permuted the feature's values (breaks the relationship between the feature and the outcome). 

### The Theory
The concept is really straightforward: 
We measure a feature's importance by calculating the increase of the model's prediction error after permuting the feature.
A feature is "important", if permuting its values increases the model error, because the model relied on the feature for the prediction;
A feature is "unimportant" if permuting its values keeps the model error unchanged, because the model ignored the feature for the prediction.
The permutation feature importance measurement was introduced for Random Forests by Breiman (2001)[^Breiman2011].
Based on this idea, Fisher, Rudin, and Dominici (2018)[^Fisher2018] proposed a model-agnostic version of the feature importance - they called it model reliance. 
They also introduce more advanced ideas about feature importance, for example a (model-specific) version that accounts for the fact that many prediction models may fit the data well. 
Their paper is worth a read. 

**The algorithm:**

Input: Trained model $\hat{f}$, feature matrix $X$, target vector $Y$, error measure $L(Y,\hat{Y})$

1. Estimate the original model error $e_{orig}(\hat{f})=L(Y,\hat{f}(X))$  (e.g. mean squared error)
2. For each feature $j\in1,\ldots,p$ do
    - Generate feature matrix $X_{perm_{j}}$ by permuting feature $X_j$ in $X$. This breaks the association between $X_j$ and $Y$.
    - Estimate error $e_{perm}=L(Y,\hat{f}(X_{perm_j}))$ based on the predictions of the permuted data.
    - Calculate permutation feature importance $FI_j=e_{perm}(\hat{f})/e_{orig}(\hat{f})$. Alternatively, the difference can be used: $FI_j=e_{perm}(\hat{f})-e_{orig}(\hat{f})$
3. Sort variables by descending $FI$.

In their paper, Fisher, Rudin, and Dominici (2018) propose to split the dataset in half and exchange the $X_j$ values of the two halves instead of permuting $X_j$. 
This is exactly the same as permuting the feature $X_j$ if you think about it. 
If you want to have a more accurate estimate, you can estimate the error of permuting $X_j$ by pairing each instance with the $X_j$ value of each other instance (except with itself). 
This gives you a dataset of size $n(n-1)$ to estimate the permutation error and it takes a big amount of computation time. 
I can only recommend using the $n(n-1)$ - method when you are serious about getting extremely accurate estimates.

### Example and Interpretation

We show examples for classification and regression. 

**Cervical cancer (Classification)**

We fit a support vector machine model to predict [cervical cancer](#cervical).
We measure the error increase by: $1-AUC$ (one minus the area under the ROC curve).
Features that are associated model error increase by a factor of 1 (= no change) were not important for predicting cervical cancer.

```{r importance-cervical, fig.cap = "The importance for each of the features in predicting cervical cancer with a support vector machine model. The importance is the factor by which the error is increased compared to the original model error."}
library('mlr')
library('iml')
data("cervical")
task = makeClassifTask(data = cervical, target = "Biopsy")
learner = makeLearner('classif.svm', predict.type = 'prob')
mod = mlr::train(learner, task)
predictor = Predictor$new(mod, data = cervical[-which(names(cervical) == "Biopsy")], y = (cervical$Biopsy == "Cancer"), class = 1)
auc_error = function(actual, predicted) 1 - Metrics::auc(actual, predicted )
importance = FeatureImp$new(predictor, loss = auc_error)
imp.dat = importance$results[c("feature", "permutation.error", "importance")]
plot(importance)
```


The feature with the highest importance was `r imp.dat[1, '..feature']` associated with an error increase of `r round(imp.dat[1,'importance'], 2)` after permutation.

**Bike rentals (Regression)**

We fit a random forest model to predict [bike rentals](#bike-data), given weather conditions and calendric information.
As error measurement we use the mean absolute error.

```{r importance-bike, fig.cap = "The importance for each of the features in predicting bike rentals with a random forest."}
data("bike")
task = makeRegrTask(data = bike, target = "cnt")
learner = makeLearner('regr.randomForest')
mod = mlr::train(learner, task)
predictor = Predictor$new(mod, data = bike[-which(names(bike) == "cnt")], y = bike$cnt)
importance = FeatureImp$new(predictor, loss = 'mae')
plot(importance) 
```

A positive aspect of using the error ratio instead of the error difference is that the feature importance measurements are comparable across different problems. 
You can see that the importance of the most important features in the bike rental prediction task is 2 to 3 times higher than in the cervical cancer prediction task. 
This indicates that the features were much more informative in the bike rental prediction task and that the bike model does a better job at predicting bike rentals compared to the cancer model at predicting cancer.


### Advantages
- Nice interpretation: Feature importance is the increase of model error when the feature's information is destroyed.
- Feature importance provides a highly compressed, global insight into the model's behavior. 


### Disadvantages
- The feature importance measure is tied to the error of the model.
This is not inherently bad, but in some cases not what you need.
In some cases you would prefer to know how much the model's output varies for one feature, ignoring what it means for the performance.
For example: You want to find out how robust your model's output is, given someone manipulates the features. 
In this case, you wouldn't be interested in how much the model performance drops given the permutation of a feature, but rather how much of the model's output variance is explained by each feature. 
Model variance (explained by the features) and feature importance correlate strongly, when the model generalizes well (i.e. it doesn't overfit).
- You need access to the actual outcome target. 
If someone only gives you the model and unlabeled data - but not the actual target-  you can't compute the permutation feature importance. 


### Software

### Alternatives

- Partial dependence based feature interaction by Greenwell et. al (2018)[^Greenwell2018]: 
They propose to measure the importance of a continuous feature by measuring how much the curves varies: 

$$i(x_j)=\sqrt{\frac{1}{k-1}\sum_{i=1}^k\left[PD_j(x_j^{(i)})-\frac{1}{k}\sum_{i=1}^kPD_j(x_j^{(i)})\right]^2}$$

And for categorical features: 

$$\left[max_i(PD_j(x_j^{(i)}))-min_i(PD_j(x_j^{(i)}))\right]/4$$


- There are many model-specific methods for feature importance:
    - For neural networks 
    -


[^Breiman2001]: Breiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1). Springer: 5–32.

[^Fisher2018]: Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. 2018. “Model Class Reliance: Variable Importance Measures for any Machine Learning Model Class, from the ‘Rashomon’ Perspective.” http://arxiv.org/abs/1801.01489.



